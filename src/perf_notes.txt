1. Varied performance from cacheline contention from the reader spinning.
2. Allowing reader to breath with sched_yield improves throughput for both blocking_ringbuff and rigtorp spsc_queue.
3. Making reader do more useful work (in our test use a larger message followed by a comparison) yieds qrius_ringbuff to outperform rigtorp by almost two times.
4. Writer batching can have a huge effect on overall throughput. In the laptop at 252 writer batch size, causes performance to improve dramatically.
5. performance varies drastically based on hardware. Switching of turbo (running at base frequency of 1.7 GHZ) vs turbo on (running worker cores at 4.7 GHZ).
6. mlock_all has a large benefit overall.
7. forcing writer and reader thread working lambda to no inline helps makes a consistent benchmark. Otherwise gcc was aggressively inlining benchmark related non-hot code path causing it to interfere with code gen of the actual hot path.
    This was from analyzing the assembly generated in perf record tool. Also noting differences in instruction cache misses from perf stats.
8. Design Notes.
        1. All are value types.
        2. Easily portable to a shared memory implementation.
        3. Throughput test are so different from latency test that do not account for cacheline contention and cache coherence traffic induced from spinning on
            shared cachelines.
            There would be considerable jitter to load latencies in such a scenario. Results perf c2c.
            This is why yielding on the reader thread generally improves the throughput.
        4. Tuned as per your usecase. My take is that there is no one size fits all concurrency queue that satisfies the highest possible throughput and/or latency with low jitter.
        5. If you need a bounded queue with a lossles reader i.e. writer blocks when the ring is full -
            API supports batched writer and reader hitting the hot shared cachelines less often increasing throughput by orders of magnitude.
            Do you need a high throughput queue or lowest latency end to end?
            Batching writes can be extremely beneficial for high throughput use cases such as processing traffic from a saturated link such as Options market data.
        6. A spinning reader on the cacheline when the writer is updating can impact throughput badly. Add proofs here from perf and perf c2c.
        7. seqlock based ringbuffer allows readers to join at anytime and back out at anytime without impacting writer performance at all.
           It also provides protection against reader reading corrupted data.
8. kesav@qbuntu:~/workspace/exercises$ sudo perf stat -C 3-4 -e LLC-load-misses,major-faults,minor-faults,ref-cycles,instructions,cache-misses,L1-icache-load-misses,L1-dcache-load-misses,L1-dcache-stores-misses,LLC-load-misses,iTLB-load-misses,dTLB-load-misses,dTLB-store-misses,bus-cycles,branch-misses,interrupts,actual-frequency,l1d.replacement,l1d_pend_miss.fb_full,l1d_pend_miss.pending,l1d_pend_miss.pending_cycles ../bin/perf_test_concurrent_queues blocking_ringbuff

Perf Test MultiCastRingBuff test iterations 800000000, write_batch=1
RingBuff capacity=134217728 elements, readers=1 threads, element_size=8 bytes
writer produced 800000000 in 17229974741 ns, wasted_writes= 0
  throughput=4.64307e+07 msgs/sec, 0.345936 GBPS, 2.97157 Gbps
reader 0 read 800000000 in 17229975855ns, wasted_reads= 17212943
  throughput=4.64307e+07 msgs/sec, 0.345936 GBPS, 2.97157 Gbps
  started -1011ns after writer
effective system throughput=4.64307e+07 msg/sec, 0.345936 GBPS, 2.97157 Gbps (influenced by latency to start reader threads & scheduling etc.)

 Performance counter stats for 'CPU(s) 3-4':

           146,143      cpu_core/LLC-load-misses/                                     (25.01%)
                 0      major-faults                                     
                 4      minor-faults                                     
    69,372,484,621      cpu_core/ref-cycles/                                          (31.28%)
    27,564,611,456      cpu_core/instructions/                                        (37.52%)
        67,680,689      cpu_core/cache-misses/                                        (43.77%)
         3,137,973      cpu_core/L1-icache-load-misses/                                     (43.77%)
       220,329,225      cpu_core/L1-dcache-load-misses/                                     (43.77%)
           140,093      cpu_core/LLC-load-misses/                                     (43.77%)
            12,745      cpu_core/iTLB-load-misses/                                     (43.76%)
            55,067      cpu_core/dTLB-load-misses/                                     (24.99%)
         1,655,974      cpu_core/dTLB-store-misses/                                     (24.99%)
    69,547,801,401      cpu_core/bus-cycles/                                          (31.24%)
       126,101,065      cpu_core/branch-misses/                                       (37.49%)
               600      interrupts                                       
                25 M    actual-frequency                                 
       746,609,500      cpu_core/l1d.replacement/                                     (37.49%)
        83,136,456      cpu_core/l1d_pend_miss.fb_full/                                     (37.49%)
    43,749,614,231      cpu_core/l1d_pend_miss.pending/                                     (37.49%) // What's happening here? Much lower for seqlock_ringbuff
    24,255,216,683      cpu_core/l1d_pend_miss.pending_cycles/                                     (37.48%)

      17.609894461 seconds time elapsed

kesav@qbuntu:~/workspace/exercises$ sudo perf stat -C 3-4 -e LLC-load-misses,major-faults,minor-faults,ref-cycles,instructions,cache-misses,L1-icache-load-misses,L1-dcache-load-misses,L1-dcache-stores-misses,LLC-load-misses,iTLB-load-misses,dTLB-load-misses,dTLB-store-misses,bus-cycles,branch-misses,interrupts,actual-frequency,l1d.replacement,l1d_pend_miss.fb_full,l1d_pend_miss.pending,l1d_pend_miss.pending_cycles ../bin/perf_test_concurrent_queues seqlock_ringbuff

Perf Test seqlock ringbuff test iterations 4000000000
RingBuff capacity=134217728 elements, readers=1 threads, element_size=8 bytes
writer produced 4000000000 in 11600449362 ns, wasted_writes= 0
  throughput=3.44814e+08 msgs/sec, 2.56907 GBPS, 22.0681 Gbps
reader 0 read 1852516352 in 11709999767ns, wasted_reads= 254
  throughput=1.582e+08 msgs/sec, 1.17868 GBPS, 10.1248 Gbps
  started -25551ns after writer
effective system throughput=1.582e+08 msg/sec, 1.17868 GBPS, 10.1248 Gbps (influenced by latency to start reader threads & scheduling etc.)

 Performance counter stats for 'CPU(s) 3-4':

           290,409      cpu_core/LLC-load-misses/                                     (24.96%)
                 0      major-faults                                     
                 3      minor-faults                                     
    48,691,339,520      cpu_core/ref-cycles/                                          (31.22%)
   133,011,704,380      cpu_core/instructions/                                        (37.48%)
       965,425,097      cpu_core/cache-misses/                                        (43.75%)
         2,504,175      cpu_core/L1-icache-load-misses/                                     (43.78%)
         6,447,648      cpu_core/L1-dcache-load-misses/                                     (43.81%)
           250,873      cpu_core/LLC-load-misses/                                     (43.84%)
            20,798      cpu_core/iTLB-load-misses/                                     (43.84%)
           104,520      cpu_core/dTLB-load-misses/                                     (25.06%)
        16,726,532      cpu_core/dTLB-store-misses/                                     (25.03%)
    48,730,161,166      cpu_core/bus-cycles/                                          (31.26%)
           614,298      cpu_core/branch-misses/                                       (37.49%)
               126      interrupts                                       
                 4 M    actual-frequency                                 
     1,573,294,526      cpu_core/l1d.replacement/                                     (37.46%)
       533,302,374      cpu_core/l1d_pend_miss.fb_full/                                     (37.43%)
       292,001,041      cpu_core/l1d_pend_miss.pending/                                     (37.40%)
       169,257,278      cpu_core/l1d_pend_miss.pending_cycles/                                     (37.40%)

      12.838127975 seconds time elapsed

kesav@qbuntu:~/workspace/exercises$

****ON VM *****

Kesav@ubsav:~/workspace/exercises/exercises$ ../bin/perf_test_concurrent_queues blocking_ringbuff

MultiCastRingBuff test_iterations=33554432, write_batch=1, read_batch=1
capacity=33554432 elements, readers=0 threads, element_size=8 bytes
writer produced 33554432 in 62367732 ns, wasted_writes= 0
  throughput=5.38009e+08 msgs/sec, 4.00848 GBPS, 34.4326 Gbps
effective system throughput=5.38009e+08 msg/sec, 4.00848 GBPS, 34.4326 Gbps (influenced by latency to start reader threads & scheduling etc.)
kesav@ubsav:~/workspace/exercises/exercises$ ../bin/perf_test_concurrent_queues blocking_ringbuff

MultiCastRingBuff test_iterations=33554432, write_batch=1, read_batch=1
capacity=33554432 elements, readers=0 threads, element_size=8 bytes
writer produced 33554432 in 64563937 ns, wasted_writes= 0
  throughput=5.19709e+08 msgs/sec, 3.87213 GBPS, 33.2613 Gbps
effective system throughput=5.19709e+08 msg/sec, 3.87213 GBPS, 33.2613 Gbps (influenced by latency to start reader threads & scheduling etc.)
kesav@ubsav:~/workspace/exercises/exercises$ ../bin/perf_test_concurrent_queues blocking_ringbuff

MultiCastRingBuff test_iterations=33554432, write_batch=1, read_batch=1
capacity=33554432 elements, readers=0 threads, element_size=8 bytes
writer produced 33554432 in 62248759 ns, wasted_writes= 0
  throughput=5.39038e+08 msgs/sec, 4.01614 GBPS, 34.4984 Gbps
effective system throughput=5.39038e+08 msg/sec, 4.01614 GBPS, 34.4984 Gbps (influenced by latency to start reader threads & scheduling etc.)
kesav@ubsav:~/workspace/exercises/exercises$ ../bin/perf_test_concurrent_queues blocking_ringbuff

MultiCastRingBuff test_iterations=33554432, write_batch=1, read_batch=1
capacity=33554432 elements, readers=0 threads, element_size=8 bytes
writer produced 33554432 in 62488788 ns, wasted_writes= 0
  throughput=5.36967e+08 msgs/sec, 4.00072 GBPS, 34.3659 Gbps
effective system throughput=5.36967e+08 msg/sec, 4.00072 GBPS, 34.3659 Gbps (influenced by latency to start reader threads & scheduling etc.)
kesav@ubsav:~/workspace/exercises/exercises$

esav@ubsav:~/workspace/exercises/exercises$ ../bin/perf_test_concurrent_queues blocking_ringbuff

MultiCastRingBuff test_iterations=33554432, write_batch=1, read_batch=1
capacity=33554432 elements, readers=0 threads, element_size=8 bytes
writer produced 33554432 in 62367732 ns, wasted_writes= 0
  throughput=5.38009e+08 msgs/sec, 4.00848 GBPS, 34.4326 Gbps
effective system throughput=5.38009e+08 msg/sec, 4.00848 GBPS, 34.4326 Gbps (influenced by latency to start reader threads & scheduling etc.)

No reader test but with no separate cachelines for in_progress and cache_reader_seqno.
kesav@ubsav:~/workspace/exercises/exercises$ ../bin/perf_test_concurrent_queues blocking_ringbuff

MultiCastRingBuff test_iterations=33554432, write_batch=1, read_batch=1
capacity=33554432 elements, readers=0 threads, element_size=8 bytes
writer produced 33554432 in 64563937 ns, wasted_writes= 0
  throughput=5.19709e+08 msgs/sec, 3.87213 GBPS, 33.2613 Gbps
effective system throughput=5.19709e+08 msg/sec, 3.87213 GBPS, 33.2613 Gbps (influenced by latency to start reader threads & scheduling etc.)
kesav@ubsav:~/workspace/exercises/exercises$ ../bin/perf_test_concurrent_queues blocking_ringbuff

MultiCastRingBuff test_iterations=33554432, write_batch=1, read_batch=1
capacity=33554432 elements, readers=0 threads, element_size=8 bytes
writer produced 33554432 in 62248759 ns, wasted_writes= 0
  throughput=5.39038e+08 msgs/sec, 4.01614 GBPS, 34.4984 Gbps
effective system throughput=5.39038e+08 msg/sec, 4.01614 GBPS, 34.4984 Gbps (influenced by latency to start reader threads & scheduling etc.)
kesav@ubsav:~/workspace/exercises/exercises$ ../bin/perf_test_concurrent_queues blocking_ringbuff

MultiCastRingBuff test_iterations=33554432, write_batch=1, read_batch=1
capacity=33554432 elements, readers=0 threads, element_size=8 bytes
writer produced 33554432 in 62488788 ns, wasted_writes= 0
  throughput=5.36967e+08 msgs/sec, 4.00072 GBPS, 34.3659 Gbps
effective system throughput=5.36967e+08 msg/sec, 4.00072 GBPS, 34.3659 Gbps (influenced by latency to start reader threads & scheduling etc.)
kesav@ubsav:~/workspace/exercises/exercises$ 

