Note: These results are from my linux vm which is a very old low power cpu.
Will present more data from an Alder Lake cpu which shows rigtop::SPSCQueue performing equally well.
And the Seqlock based ringbuffer runs much faster than others on Alder Lake.

Seqlock based ringbuffer optimized for writes
    writer throughput = 376M msgs/second.
    reader throughput = 187M msgs/second (writer goes too fast for the reader)

    seqlock ringbuff test_iterations=800000000                                                               
    starting writer   target = 800000000, core_affinity = 3                             
    starting reader 0 target = 800000000, core_affinity = 4                                                  
    capacity=33554432 elements, readers=1 threads, element_size=8 bytes                                      
    writer produced 800000000 in 2128617430 ns, wasted_writes= 0                        
    throughput=3.75831e+08 msgs/sec, 2.80016 GBPS, 24.0532 Gbps, avg_latency=2.66077ns                     
    reader 0 read 397346816 in 2132206965ns, wasted_reads= 7767                                              
    throughput=1.86355e+08 msgs/sec, 1.38845 GBPS, 11.9267 Gbps, avg_latency=5.36611ns                     
    started -57572ns after writer                                                                          
    effective system throughput=1.8636e+08 msg/sec, 1.38849 GBPS, 11.927 Gbps, avg_latency=5.36597ns (influenced by latency to start reader threads & scheduling etc.)

Ringbuffer that allows reader to catch up.
    writer throughput = 417 msgs/second.
    reader throughput = 402M msgs/second.

    MultiCastRingBuff test_iterations=800000000, write_batch=1, read_batch=1
    capacity=33554432 elements, readers=1 threads, element_size=8 bytes
    writer produced 800000000 in 1917642299 ns, wasted_writes= 11334517
    throughput=4.17179e+08 msgs/sec, 3.10823 GBPS, 26.6995 Gbps, avg_latency=2.39705ns
    reader 0 read 800000000 in 1986208356ns, wasted_reads= 8453
    throughput=4.02777e+08 msgs/sec, 3.00093 GBPS, 25.7778 Gbps, avg_latency=2.48276ns
    started -59233ns after writer
    effective system throughput=4.02789e+08 msg/sec, 3.00102 GBPS, 25.7785 Gbps, avg_latency=2.48269ns (influenced by latency to start reader threads & scheduling etc.)

folly::SingleProduceSingleConsumer 
    writer throughput = 134M msgs/second.
    reader throughput = 134M msgs/second.

    folly::ProducerConsumerQueue test_iterations=800000000                                                   
    starting writer   target = 800000000, core_affinity = 3                                                  
    starting reader 0 target = 800000000, core_affinity = 4                                                  
    capacity=33554432 elements, readers=1 threads, element_size=8 bytes                                      
    writer produced 800000000 in 5957096251 ns, wasted_writes= 0 
    throughput=1.34294e+08 msgs/sec, 1.00057 GBPS, 8.59479 Gbps, avg_latency=7.44637ns                     
    reader 0 read 800000000 in 5966416190ns, wasted_reads= 293691                       
    throughput=1.34084e+08 msgs/sec, 0.999002 GBPS, 8.58137 Gbps, avg_latency=7.45802ns                    
    started -19689ns after writer                                                                          
    effective system throughput=1.34084e+08 msg/sec, 0.999006 GBPS, 8.58139 Gbps, avg_latency=7.458ns (influenced by latency to start reader threads & scheduling etc.)  

rigtorp::SPSCQueue
The test results in writer going too fast such that it spends more time contending with reader cacheline to make progress.
This is clear from wasted_writes stat below.
I will present the actual number from an Alder Lake later which shows this ringbuffer performing extremely well.

    rigtorp::SPSCQueue, test_iterations=800000000
    capacity=33554432 elements, readers=1 threads, element_size=8 bytes
    writer produced 800000000 in 2481569451 ns, wasted_writes= 0
    throughput=3.22377e+08 msgs/sec, 2.40189 GBPS, 20.6321 Gbps, avg_latency=3.10196ns
    reader 0 read 800000000 in 2481592746ns, wasted_reads= 10676016
    throughput=3.22374e+08 msgs/sec, 2.40187 GBPS, 20.6319 Gbps, avg_latency=3.10199ns
    started -23326ns after writer
    effective system throughput=3.22377e+08 msg/sec, 2.40189 GBPS, 20.6321 Gbps, avg_latency=3.10196ns (influenced by latency to start reader threads & scheduling etc.)
