Note: These results are from my linux vm which is a very old low power cpu (Cofee Lake, 35W)
Currently performing more experiments on my Alder Lake laptop which shows rigtop::SPSCQueue performing equally well.
Will present those results soon.
The Seqlock based ringbuffer runs faster than others on Alder Lake.

Seqlock based ringbuffer optimized for writes
    writer throughput = 421M msgs/second.
    reader throughput = 175M msgs/second (writer goes too fast for the reader)

    seqlock ringbuff test_iterations=800000000
    capacity=33554432 elements, readers=1 threads, element_size=8 bytes
    writer produced 800000000 in 1900812526 ns, wasted_writes= 0
    throughput=4.20873e+08 msgs/sec, 3.13575 GBPS, 26.9358 Gbps, avg_latency=2.37602ns
    reader 0 read 363792384 in 2080510977ns, wasted_reads= 7329
    throughput=1.74857e+08 msgs/sec, 1.30279 GBPS, 11.1909 Gbps, avg_latency=5.71895ns
    started -56689ns after writer
    effective system throughput=1.74862e+08 msg/sec, 1.30282 GBPS, 11.1912 Gbps, avg_latency=5.7188ns (influenced by latency to start reader threads & scheduling etc.)

Ringbuffer that allows reader to catch up.
    writer throughput = 417M msgs/second.
    reader throughput = 402M msgs/second.

    MultiCastRingBuff test_iterations=800000000, write_batch=1, read_batch=1
    capacity=33554432 elements, readers=1 threads, element_size=8 bytes
    writer produced 800000000 in 1917642299 ns, wasted_writes= 11334517
    throughput=4.17179e+08 msgs/sec, 3.10823 GBPS, 26.6995 Gbps, avg_latency=2.39705ns
    reader 0 read 800000000 in 1986208356ns, wasted_reads= 8453
    throughput=4.02777e+08 msgs/sec, 3.00093 GBPS, 25.7778 Gbps, avg_latency=2.48276ns
    started -59233ns after writer
    effective system throughput=4.02789e+08 msg/sec, 3.00102 GBPS, 25.7785 Gbps, avg_latency=2.48269ns (influenced by latency to start reader threads & scheduling etc.)

folly::SingleProduceSingleConsumer
    writer throughput = 134M msgs/second.
    reader throughput = 134M msgs/second.

    folly::ProducerConsumerQueue test_iterations=800000000
    starting writer   target = 800000000, core_affinity = 3
    starting reader 0 target = 800000000, core_affinity = 4
    capacity=33554432 elements, readers=1 threads, element_size=8 bytes
    writer produced 800000000 in 5957096251 ns, wasted_writes= 0
    throughput=1.34294e+08 msgs/sec, 1.00057 GBPS, 8.59479 Gbps, avg_latency=7.44637ns
    reader 0 read 800000000 in 5966416190ns, wasted_reads= 293691
    throughput=1.34084e+08 msgs/sec, 0.999002 GBPS, 8.58137 Gbps, avg_latency=7.45802ns
    started -19689ns after writer
    effective system throughput=1.34084e+08 msg/sec, 0.999006 GBPS, 8.58139 Gbps, avg_latency=7.458ns (influenced by latency to start reader threads & scheduling etc.)

rigtorp::SPSCQueue
The test results in writer going too fast such that it spends more time contending with reader cacheline to make progress.
This is clear from wasted_writes stat below.
I will present the actual number from an Alder Lake later which shows this ringbuffer performing extremely well.

    rigtorp::SPSCQueue, test_iterations=800000000
    capacity=33554432 elements, readers=1 threads, element_size=8 bytes
    writer produced 800000000 in 2481569451 ns, wasted_writes= 0
    throughput=3.22377e+08 msgs/sec, 2.40189 GBPS, 20.6321 Gbps, avg_latency=3.10196ns
    reader 0 read 800000000 in 2481592746ns, wasted_reads= 10676016
    throughput=3.22374e+08 msgs/sec, 2.40187 GBPS, 20.6319 Gbps, avg_latency=3.10199ns
    started -23326ns after writer
    effective system throughput=3.22377e+08 msg/sec, 2.40189 GBPS, 20.6321 Gbps, avg_latency=3.10196ns (influenced by latency to start reader threads & scheduling etc.)